{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 4: Word2Vec\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *L*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Imane Benkamoun*\n",
    "* *Gabriel Yehouda Gozlan*\n",
    "* *Mathis Le Dortz*\n",
    "* *Hervé Sérandour*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 4 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from utils import *\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, we'll do a little data exploration, to get an idea of the pre-processing required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /ix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sc.textFile(\"/ix/model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_and_vector(row):\n",
    "    \n",
    "    list_row = row.split(\" \")\n",
    "    word = list_row[0]\n",
    "    vector = np.array([float(coord) for coord in list_row[1:]])\n",
    "    \n",
    "    return (word, vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_and_vectors = vectors.map(get_word_and_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = (words_and_vectors.map(lambda row: row[0]).collect())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in word_list if word[1:].lower()!=word[1:]][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_remove(course):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove punctuation, one-letter words and spaces\n",
    "    \"\"\"\n",
    "   \n",
    "    without_punctuation = re.sub(r'[^\\w\\s]', ' ', course[\"description\"])\n",
    "    \n",
    "    words_to_dell = r'\\b(?:' + '|'.join(map(re.escape, stopwords)) + r')\\b'\n",
    "    without_stopwords = re.sub(words_to_dell, ' ', without_punctuation)\n",
    "    \n",
    "    without_one_letter = re.sub(r'\\b\\w{1}\\b', ' ', without_stopwords) \n",
    "\n",
    "    without_useless_spaces = re.sub(r'\\s+', ' ', without_one_letter).strip()\n",
    "    \n",
    "    \n",
    "    return without_useless_spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_description = [to_remove(course) for course in courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_description[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(courses_description):\n",
    "    \n",
    "    unique_words_set = set()\n",
    "    \n",
    "    # course_description is a list of course descriptions\n",
    "    unique_in_course = [set(description.split()) for description in courses_description] \n",
    "\n",
    "    for description_cleaned in unique_in_course:\n",
    "        unique_words_set.update(description_cleaned)\n",
    "    \n",
    "    return list(unique_words_set)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_list = unique_words(courses_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.12 : Clustering word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/ix/model.txt'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_vectors(unique_words_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function produces a word: vector dictionary for words in the model\n",
    "    that are also in the course description dataset, \n",
    "    otherwise, a list of words without associated vectors is created.\n",
    "    \"\"\"\n",
    "    words_as_keys = {}\n",
    "    words_not_in_model = []\n",
    "    \n",
    "    for word in unique_words_list:\n",
    "        try:\n",
    "            words_as_keys[word] = model.get_vector(word)\n",
    "        except KeyError:\n",
    "            words_not_in_model.append(word)\n",
    "    \n",
    "    return words_as_keys, words_not_in_model\n",
    "    \n",
    "\n",
    "def set_default_vector(words_as_keys, words_not_in_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function assigns a default vector to words in the process dataset that are not \n",
    "    in the model.\n",
    "    This default vector corresponds to the average of the vectors in the process dataset, \n",
    "    and not in the entire model, to get a vector that better matches our data.\n",
    "    \"\"\"\n",
    "    vectors_for_mean = list(words_as_keys.values())\n",
    "    default_vector = np.mean(vectors_for_mean, axis=0)\n",
    "    \n",
    "    for word in words_not_in_model:\n",
    "        words_as_keys[word] = default_vector\n",
    "    \n",
    "    return words_as_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    \n",
    "    norm=np.linalg.norm(vector)\n",
    "    if norm==0:\n",
    "        return\n",
    "    return vector/norm\n",
    "\n",
    "\n",
    "def normalize_all_vectors(all_words_and_vectors):\n",
    "    return {word: normalize(vector) for word, vector in all_words_and_vectors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(words_as_keys, words_not_in_model) = words_to_vectors(unique_words_list)\n",
    "all_words_and_vectors = set_default_vector(words_as_keys, words_not_in_model)\n",
    "\n",
    "# We have chosen to post-normalize\n",
    "# so that the default vector takes into account the weight of the words in the processed dataset\n",
    "\n",
    "all_words_and_vectors = normalize_all_vectors(all_words_and_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_and_vectors[\"Studio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_for_opt_k(all_words_and_vectors):\n",
    "    \n",
    "    \"\"\"\n",
    "    We use Elbow's algorithm to get an idea of the optimal number of clusters.\n",
    "    Knowing that visually the k_opt corresponds to the k for which the slope\n",
    "    of the curve below no longer varies significantly\n",
    "    \"\"\"\n",
    "    wcss = [] \n",
    "    for i in range(1, 20):\n",
    "        data = list(all_words_and_vectors.values())\n",
    "        kmeans = KMeans(n_clusters = i)\n",
    "        kmeans.fit(data) \n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "    return wcss\n",
    "\n",
    "wcss = plot_for_opt_k(all_words_and_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.plot(range(1, 20), wcss, marker='o')\n",
    "ax.set_xlabel(\"Number of clusters\")\n",
    "ax.set_ylabel(\"WCSS\")\n",
    "ax.set_title(\"Find the optimal k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_k = 15 # we choose a number of clusters equal to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(all_words_and_vectors):\n",
    "    \n",
    "    data = list(all_words_and_vectors.values())\n",
    "    kmeans = KMeans(n_clusters = opt_k).fit(data)\n",
    "    \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = clustering(all_words_and_vectors)\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_for_a_cluster(model, unique_words_list, center, step):\n",
    "    similars = []\n",
    "    similar_with_all = []\n",
    "    nb = 0\n",
    "    while len(similars) < 10:\n",
    "        \n",
    "        # we look at all the steps to see if we can add words, with the condition they are also in our dataset\n",
    "        # As model.similar_vector sends the vectors in descending order of similarity, \n",
    "        # we know that we'll get the most similar vectors first, which are also in the processed dataset\n",
    "\n",
    "        similar_with_all = [ele[0] for ele in model.similar_by_vector(center, topn=nb+step)[nb:nb+step]]\n",
    "        for word in similar_with_all:\n",
    "            if (word in unique_words_list) and (word not in similars):\n",
    "                similars.append(word)\n",
    "        nb+=step\n",
    "    return similars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_10_for_each_cluster(model, unique_words_list, centers):\n",
    "    for i, center in enumerate(centers):\n",
    "        top10 = top_10_for_a_cluster(model, unique_words_list, center, 100)\n",
    "        print(f\"**Cluster number : {i}**\")\n",
    "        for word in top10:\n",
    "            print(word)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_10_for_each_cluster(model, unique_words_list, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_for_clusters(all_words_and_vectors, unique_words_list, model, centers):\n",
    "    \n",
    "    \"\"\"\n",
    "    For the cluster labels, we choose the closest word in terms of similarity, \n",
    "    but in the entire model this time.\n",
    "    \"\"\"\n",
    "    for i, center in enumerate(centers[:10]):\n",
    "        label =  model.similar_by_vector(center, topn=1)[0][0]\n",
    "        print(f\"Cluster number {i} - Label : {label}\")\n",
    "        print()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_for_clusters(all_words_and_vectors, unique_words_list, model, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec and LSI have cluster labels on similar subjects, science and methods, the notable differences are related to the presence of a cluster containing names (label: \"Miller\") for Word2Vec, which is not present in\n",
    "LSI, but also of a cluster containing numbers (label: \"245\"), not present in LSI either.\n",
    "\n",
    "It's probably because Word2Vec captures meaning better, even if the names or numbers don't appear in the same context, Word2Vec managed to make the association, which is a priori more complicated for LSI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.13 : Document similarity search¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part on TF-IDF calculation has been taken from the first notebook,\n",
    "but has been adapted for this one, since the pre-processing has been different.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "corpus = [course.split() for course in courses_description]\n",
    "\n",
    "\n",
    "# Create a vocabulary and term-to-index mapping\n",
    "vocabulary = set(word for text in corpus for word in text)\n",
    "vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create a document-to-index mapping\n",
    "doc_Id_to_index = {course['courseId'] : idx  for idx, course in enumerate(courses)}\n",
    "doc_index_to_ID = {idx : course['courseId']  for idx, course in enumerate(courses)}\n",
    "\n",
    "len(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionaries.pkl', 'rb') as file:\n",
    "    vocab_index, doc_index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Compute term frequencies (TF)\n",
    "term_frequencies = defaultdict( lambda : defaultdict(int))\n",
    "for doc_id, text in enumerate(corpus):\n",
    "    for word in text:\n",
    "        term_frequencies[doc_id][word] += 1\n",
    "\n",
    "# Compute document frequencies (DF)\n",
    "df = defaultdict(int)\n",
    "for word in vocabulary:\n",
    "    for doc_id in term_frequencies:\n",
    "        if word in term_frequencies[doc_id]:\n",
    "            df[word] += 1\n",
    "\n",
    "# Compute the inverse document frequencies (IDF)\n",
    "N = len(courses)\n",
    "idf = {word: math.log(N / df[word]) for word in vocabulary}\n",
    "\n",
    "# Compute the TF-IDF matrix\n",
    "rows, cols, data = [], [], []\n",
    "for doc_id in term_frequencies:\n",
    "    for word in term_frequencies[doc_id]:\n",
    "        rows.append(vocab_index[word])\n",
    "        cols.append(doc_id)\n",
    "        data.append(term_frequencies[doc_id][word] * idf[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_course_to_vector(course):\n",
    "    \n",
    "    \"\"\"\n",
    "    For a given course, \n",
    "    we calculate the associated total vector, using the word weights given by TD-IDF \n",
    "    and the normalized vectors calculated above\n",
    "    \"\"\"\n",
    "    doc_id = doc_Id_to_index[course[\"courseId\"]]\n",
    "    cleaned_course = to_remove(course).split()\n",
    "    \n",
    "    vectors_for_mean = []\n",
    "    for word in cleaned_course:\n",
    "        vectors_for_mean.append((term_frequencies[doc_id][word] * idf[word])*all_words_and_vectors[word])\n",
    "    \n",
    "    return (doc_id, np.mean(vectors_for_mean, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_tf_idf_for_word(word):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is useful for obtaining \n",
    "    the average weight of a word over the entire corpus, \n",
    "    we have chosen to use the average TF-IDF to translate the average importance of the word in the corpus\n",
    "    \"\"\"\n",
    "    nb_doc = len(courses)\n",
    "    weights_for_word = []\n",
    "    for doc_id in range(nb_doc):\n",
    "        if word in to_remove(courses[doc_id]).split():\n",
    "            weights_for_word.append((term_frequencies[doc_id][word] * idf[word]))\n",
    "    \n",
    "    n = len(weights_for_word)\n",
    "    if n:\n",
    "        return np.sum(weights_for_word)/n\n",
    "    else: # if the word is not in the processed dataset\n",
    "        return 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_query_to_vector(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    The query vector is calculated from the average of the vectors for each word, \n",
    "    weighted by their average TF-IDF over the corpus \n",
    "    \"\"\"\n",
    "    query_word_list = query.split()\n",
    "    vectors_for_mean = []\n",
    "    for word in query_word_list:\n",
    "        if word in unique_words_list:\n",
    "            vectors_for_mean.append((average_tf_idf_for_word(word))*all_words_and_vectors[word])\n",
    "    \n",
    "    return np.mean(vectors_for_mean, axis=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    '''\n",
    "    Computes the cosine similarity between two vectors.\n",
    "    '''\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm1 = norm(vector1)\n",
    "    norm2 = norm(vector2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_courses_for_query(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns a global list of courses related to the query\n",
    "    in descending order of similarity\n",
    "    \"\"\"\n",
    "    query_vector = convert_query_to_vector(query)\n",
    "    similarity_with_courses = []\n",
    "    for course in courses:\n",
    "        (doc_id, course_vector) = convert_course_to_vector(course)\n",
    "        similarity_with_courses.append((doc_id, cosine_similarity(query_vector, course_vector)))\n",
    "        \n",
    "    return sorted(similarity_with_courses, key=lambda x: x[1], reverse=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(query):\n",
    "    top_five = top_courses_for_query(query)[:5] # because the list is sorted in descending order of similarity\n",
    "    print(f\"Here are the top5 courses most closely related to the query - {query} :\\n\")\n",
    "    for i, doc_index in enumerate([ele[0] for ele in top_five]):\n",
    "        course_id = courses[doc_index][\"courseId\"]\n",
    "        course_name = courses[doc_index][\"name\"]\n",
    "        similarity = top_five[i][1]\n",
    "        print(f\"{course_id} : {course_name} - Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Markov chains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Markov chains\" in  courses[doc_Id_to_index[\"MGT-602\"]][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Markov chains\" in  courses[doc_Id_to_index[\"COM-512\"]][\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison with LSI, We obtain almost the same list of courses for the query “Markov chains”. Only one course is different (COM-512 for LSI vs. MGT-602 for Word2Vec), and the course order and similarity values also differ. In fact, COM-512 isn't even in the top10 courses with Word2Vec, even though it contains “Markov chains” in its description (as you can see). This is undoubtedly because Word2Vec captures the relationships between words, and therefore captures the link between mathematical models for the supply chain and Markov chains in particular, which LSI does less well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Markov\" in  courses[doc_Id_to_index[\"EE-516\"]][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(courses[doc_Id_to_index[\"EE-516\"]][\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For VSM, We get the same order of courses as with Word2Vec, except that here again, only one course differs (the 5th: MGT-602 for Word2Vec versus EE-516 for the vector space model). We can see that EE-516 talks about HMMs (Hidden Markov Models) which are an extension of Markov chains. However, it seems that VSM cited EE-516 only because it contains\n",
    "the term “Markov”, without making the link between HMMs and Markov chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Facebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"social\" in courses[doc_Id_to_index[\"HUM-432(a)\"]][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"media\" in courses[doc_Id_to_index[\"HUM-432(a)\"]][\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison with LSI, again, we get the same course with the greatest similarity. Only two courses are different between LSI and Word2Vec. Where “EE-552”, which is present with LSI but not with Word2Vec, remains relevant, because it concerns “Media Security”, the “HUM-432(a)” course turns out to be a misinterpretation for LSI, since this course is not directly related to Facebook, rather, it seems to have been chosen by LSI because the description contains the word “social” (but not “media”, so not “social media” either).\n",
    "\n",
    "As for VSM, here again, we obtain the same course with the most similarities, but for the other 4 courses, the results don't seem relevant. Where Markov Chains was in the course description, for Facebook it's different, and VSM is clearly less relevant.\n",
    "\n",
    "To conclude this analysis, from what we can see it seems that the method that best captures the meaning of words is Word2Vec. LSI also manages to make relevant associations, whereas VSM focuses too much on the presence of the words themselves, without paying attention to meaning. And this is consistent with the course, since Word2Vec was created to capture the meaning of words, whereas VSM is more effective at labelling label documents in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.14: Document similarity search with outside terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_convert_query_to_vector(model, query):\n",
    "    \n",
    "    \"\"\"\n",
    "    We add the model as a parameter, to allow this time \n",
    "    to use words outside the corpus, but within the model.\n",
    "    The TF-IDF of a word outside the corpus is set to 1 by default.\n",
    "    \"\"\"\n",
    "    query_word_list = query.split()\n",
    "    vectors_for_mean = []\n",
    "    for word in query_word_list:\n",
    "        if word in unique_words_list:\n",
    "            vectors_for_mean.append((average_tf_idf_for_word(word))*all_words_and_vectors[word])\n",
    "        else:\n",
    "            vectors_for_mean.append(normalize(model.get_vector(word)))\n",
    "    \n",
    "    return np.mean(vectors_for_mean, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_top_courses_for_query(model, query):\n",
    "    query_vector = generalized_convert_query_to_vector(model, query)\n",
    "    similarity_with_courses = []\n",
    "    for course in courses:\n",
    "        (doc_id, course_vector) = convert_course_to_vector(course)\n",
    "        similarity_with_courses.append((doc_id, cosine_similarity(query_vector, course_vector)))\n",
    "        \n",
    "    return sorted(similarity_with_courses, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_display(model, query):\n",
    "    top_five = generalized_top_courses_for_query(model, query)[:5]\n",
    "    print(f\"Here are the top5 courses most closely related to the query - {query} :\\n\")\n",
    "    for i, doc_index in enumerate([ele[0] for ele in top_five]):\n",
    "        course_id = courses[doc_index][\"courseId\"]\n",
    "        course_name = courses[doc_index][\"name\"]\n",
    "        similarity = top_five[i][1]\n",
    "        print(f\"{course_id} : {course_name} - Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_display(model, \"MySpace Orkut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"MySpace\" in  courses[doc_Id_to_index[\"EE-727\"]][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Facebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Facebook\" in  courses[doc_Id_to_index[\"EE-727\"]][\"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same for Facebook and MySpace Orkut, with the exception of one course, which is different, and a different order and values for the similarities. As for the difference in value, Facebook is notably present in the description of the first course, which is not the case for MySpace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_display(model, \"coronavirus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These courses are all in the biology section and talk globally about infections, diseases, viruses or bacteria, so it's consistent with coronavirus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
