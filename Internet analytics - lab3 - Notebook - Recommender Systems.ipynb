{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 — recommender systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from random import randint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-rw-rw-+  3 mkhorasa supergroup  745096004 2024-04-04 16:54 /ix/ml-20m/genome-scores.txt\r\n",
      "-rw-rw-rw-+  3 mkhorasa supergroup      40652 2024-04-04 16:54 /ix/ml-20m/genome-tags.txt\r\n",
      "-rw-rw-rw-+  3 mkhorasa supergroup    2538070 2024-04-04 16:54 /ix/ml-20m/movies.txt\r\n",
      "-rw-rw-rw-+  3 mkhorasa supergroup 1493457002 2024-04-04 16:54 /ix/ml-20m/ratings.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /ix/ml-20m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"movieId\": 2, \"userId\": 1, \"timestamp\": 1112486027, \"rating\": 3.5}\r\n",
      "{\"movieId\": 29, \"userId\": 1, \"timestamp\": 1112484676, \"rating\": 3.5}\r\n",
      "{\"movieId\": 32, \"userId\": 1, \"timestamp\": 1112484819, \"rating\": 3.5}\r\n",
      "{\"movieId\": 47, \"userId\": 1, \"timestamp\": 1112484727, \"rating\": 3.5}\r\n",
      "{\"movieId\": 50, \"userId\": 1, \"timestamp\": 1112484580, \"rating\": 3.5}\r\n",
      "{\"movieId\": 112, \"userId\": 1, \"timestamp\": 1094785740, \"rating\": 3.5}\r\n",
      "{\"movieId\": 151, \"userId\": 1, \"timestamp\": 1094785734, \"rating\": 4.0}\r\n",
      "{\"movieId\": 223, \"userId\": 1, \"timestamp\": 1112485573, \"rating\": 4.0}\r\n",
      "{\"movieId\": 253, \"userId\": 1, \"timestamp\": 1112484940, \"rating\": 4.0}\r\n",
      "{\"movieId\": 260, \"userId\": 1, \"timestamp\": 1112484826, \"rating\": 4.0}\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /ix/ml-20m/ratings.txt | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3.5 : Basic statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_rdd = sc.textFile(\"/ix/ml-20m/ratings.txt\") # First of all, we transform the ratings file on hdfs into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"movieId\": 2, \"userId\": 1, \"timestamp\": 1112486027, \"rating\": 3.5}',\n",
       " '{\"movieId\": 29, \"userId\": 1, \"timestamp\": 1112484676, \"rating\": 3.5}',\n",
       " '{\"movieId\": 32, \"userId\": 1, \"timestamp\": 1112484819, \"rating\": 3.5}',\n",
       " '{\"movieId\": 47, \"userId\": 1, \"timestamp\": 1112484727, \"rating\": 3.5}',\n",
       " '{\"movieId\": 50, \"userId\": 1, \"timestamp\": 1112484580, \"rating\": 3.5}',\n",
       " '{\"movieId\": 112, \"userId\": 1, \"timestamp\": 1094785740, \"rating\": 3.5}',\n",
       " '{\"movieId\": 151, \"userId\": 1, \"timestamp\": 1094785734, \"rating\": 4.0}',\n",
       " '{\"movieId\": 223, \"userId\": 1, \"timestamp\": 1112485573, \"rating\": 4.0}',\n",
       " '{\"movieId\": 253, \"userId\": 1, \"timestamp\": 1112484940, \"rating\": 4.0}',\n",
       " '{\"movieId\": 260, \"userId\": 1, \"timestamp\": 1112484826, \"rating\": 4.0}']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cloudera/parcels/CDH-7.1.9-1.cdh7.1.9.p0.44702451/lib/spark/python/pyspark/sql/session.py:366: UserWarning:\n",
      "\n",
      "Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the visualisations in this section, \n",
    "# we decided to use the power of spark thanks to spark dataframes, and then use tools like plotly.\n",
    "\n",
    "ratings_dataframe = spark.createDataFrame(ratings_rdd.map(json.loads))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(spark_df, length_interval, nb_element, name_element):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates a bar chart whose x-axis includes all possible values \n",
    "    from an interval defined by its length and a random starting point in the definition space (movieId or userId here),\n",
    "    and the y-axis includes the set of ratings for the x-interval.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    random_element = randint(0, (nb_element-length_interval-1))\n",
    "\n",
    "    data_to_plot = spark_df.orderBy(name_element, ascending=True).toPandas().iloc[random_element:random_element+length_interval]\n",
    "\n",
    "\n",
    "    fig = px.bar(data_to_plot, x=name_element, y=\"nb_rating\")\n",
    "\n",
    "    fig.update_layout(yaxis_title=\"Number of rating\")\n",
    "    fig.update_layout(title=f\"Number of rating for each {name_element}\")\n",
    "\n",
    "\n",
    "    fig.show(renderer='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|userId|nb_rating|\n",
      "+------+---------+\n",
      "|    26|       61|\n",
      "|    29|      177|\n",
      "|   474|      141|\n",
      "|   964|      227|\n",
      "|  1677|      154|\n",
      "+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We obtain the dataframe of the number of ratings per userId\n",
    "\n",
    "user_rating_data = ratings_dataframe.groupBy(\"userId\").agg({\"rating\": \"count\"}).withColumnRenamed(\"count(rating)\", \"nb_rating\")\n",
    "user_rating_data.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138493\n"
     ]
    }
   ],
   "source": [
    "nb_users = user_rating_data.count() # number of unique users\n",
    "print(nb_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(user_rating_data, 300, nb_users, \"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the same thing as above, but for movies\n",
    "\n",
    "movie_rating_data = (ratings_dataframe\n",
    "                     .groupBy(\"movieId\")\n",
    "                     .agg({\"rating\": \"count\"})\n",
    "                     .withColumnRenamed(\"count(rating)\", \"nb_rating\")\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_movies = movie_rating_data.count()\n",
    "print(nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualization(movie_rating_data, 300, nb_movies, \"movieId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a clear and important imbalance between the two visualisations: \n",
    "\n",
    "- The userId view shows visually that all users have voted\n",
    "\n",
    "- The view by movie shows a lot of ‘holes’, and suggests that many movies have not been rated.\n",
    "\n",
    "(Later on in this lab, we can see that the number of users without a rating is 34, whereas for movies it is 105 403)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3.6 : Partitioning the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal ./my-ratings.txt # Add the file to my home directory on hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat my-ratings.txt # Below are my ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ratings_rdd = sc.textFile(\"my-ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_rdd = ratings_rdd.union(my_ratings_rdd) # My file is combined with the ratings file of other users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, the file is split into a training set containing 16 million lines and a validation set containing 4 million lines, giving a 4/5 and 1/5 split by dividing the combined_rdd by the last timestamp digit. This can be considered a good way of doing things, as there is no bias in this way. \n",
    "\n",
    "For example, if we had taken the first 4/5 lines, there could have been a time bias, perhaps because a style of film is more popular at a given period, and therefore at a given timestamp range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = combined_rdd.map(json.loads).filter(lambda row: str(row[\"timestamp\"])[-1] in map(str, range(8))) \n",
    "# The last digit of the timestamp is between 0 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = combined_rdd.map(json.loads).filter(lambda row: str(row[\"timestamp\"])[-1] in map(str, range(8, 10)))\n",
    "# The last digit of the timestamp is between 8 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3.7 : Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_movies and N_users are respectively the number of unique films and users in the dataset. They will be useful later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_movies = (training_set.map(lambda row: (row[\"movieId\"], 0))\\\n",
    "                       .reduceByKey(lambda x, y: x+y)\\\n",
    "                       .count())\n",
    "print(f\"\"\"The number of unique films in the training set is : {N_movies}\"\"\")  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_users = (training_set.map(lambda row: (row[\"userId\"], 0))\\\n",
    "                       .reduceByKey(lambda x, y: x+y)\\\n",
    "                       .count())\n",
    "\n",
    "print(f\"\"\"The number of unique users in the training set is : {N_users}\"\"\")     \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_sum = (training_set\n",
    "              .map(lambda row: (row[\"rating\"], 1))\n",
    "              .reduce(lambda row1, row2 : (row1[0]+row2[0], row1[1]+row2[1]))\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rating_sum) # We obtain the tuple (total rating, number of ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_r = rating_sum[0]\n",
    "N = rating_sum[1]\n",
    "\n",
    "\n",
    "\n",
    "mu = global_r/N\n",
    "print(f\"\"\"The average rating for all films and users is : {mu}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = training_set.map(lambda row: (row[\"userId\"], (1, row[\"rating\"]-mu)))\n",
    "\n",
    "# For each user, we calculate the tuple (number of films rated, sum of the user's ratings).\n",
    "user_data_grouped_sum = user_data.reduceByKey(lambda row1, row2: (row1[0]+row2[0], row1[1]+row2[1]))\n",
    "\n",
    "# Finally, for each user, the ratio of the previous tuple is calculated to obtain its bias. \n",
    "user_biases = user_data_grouped_sum.map(lambda user: (user[0], user[1][1]/user[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_userId = (training_set\n",
    "              .map(lambda row: row[\"userId\"])\n",
    "              .reduce(lambda x, y: max(x, y))\n",
    "             )\n",
    "\n",
    "\n",
    "print(f\"\"\"There are {max_userId - N_users} users who have no ratings.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed just before, some users don't have ratings, so we need to give them a default bias. We have chosen to use the average bias of all the users in the training_set, which we call avg_user_biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biases_sum = user_biases.reduce(lambda user1, user2: (_, user1[1]+user2[1]))[1]\n",
    "avg_user_biases = user_biases_sum/(user_biases.count())\n",
    "  \n",
    " \n",
    "\n",
    "default_value_user_bias = [(i, avg_user_biases) for i in range(1, N_users+1)] # Here we use avg_user_biases as default bias\n",
    "default_value_user_bias_rdd = sc.parallelize(default_value_user_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biases_final = (user_biases\n",
    "                      .rightOuterJoin(default_value_user_bias_rdd) \n",
    "                      .map(lambda row: (row[0], row[1][0] if row[1][0]!=None else row[1][1])))\n",
    "\n",
    "# The default value is used if the user is not in the user_biases rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_biases_final.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_movieId = (training_set\n",
    "              .map(lambda row: row[\"movieId\"])\n",
    "              .reduce(lambda x, y: max(x, y))\n",
    "             )\n",
    "\n",
    "\n",
    "print(f\"\"\"There are {max_movieId - N_movies} movies who have no ratings.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an RDD containing all the tuples (movieId, bias associated with the user who rated the movie).\n",
    "movies_users_biases = (training_set\n",
    "                       .map(lambda row: (row[\"userId\"], row[\"movieId\"]))\n",
    "                       .join(user_biases_final)\n",
    "                       .map(lambda row: (row[1][0], row[1][1]))\n",
    "                      ) \n",
    "\n",
    "# This RDD contains all the tuples (movieId, sum of the biases of all the users who rated the movieId).\n",
    "groupUsersByMovie = (movies_users_biases.groupByKey()\n",
    "                      .mapValues(sum))\n",
    "                     \n",
    "\n",
    "    \n",
    "# This step is necessary to subtract user bias from movie bias, as specified in the formula.\n",
    "                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = training_set.map(lambda row: (row[\"movieId\"], (1, row[\"rating\"]-mu)))\n",
    "\n",
    "# For each movie, we calculate the tuple (number of times the movie has been rated, sum of the movie's ratings).\n",
    "movie_data_grouped_sum = (movie_data\n",
    "                          .reduceByKey(lambda row1, row2: (row1[0]+row2[0], row1[1]+row2[1]))\n",
    "                          .join(groupUsersByMovie)\n",
    "                          .map(lambda row: (row[0], (row[1][0][0], row[1][0][1], row[1][1])))\n",
    "                         )\n",
    "\n",
    "# Finally, for each movie, the ratio of the previous tuple is calculated to obtain its bias, \n",
    "# also we subtracte the bias of the users who rated the film, calculated above.\n",
    "\n",
    "movie_biases = movie_data_grouped_sum.map(lambda movie: (movie[0], (movie[1][1]-movie[1][2])/movie[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with what we did with the users, \n",
    "# there are some movies that have no bias, so we set them to the default value avg_movie_biases.\n",
    "\n",
    "sum_movie_biases = movie_biases.reduce(lambda x, y: (_, x[1]+y[1]))[1]\n",
    "avg_movie_biases = sum_movie_biases/(movie_biases.count())\n",
    "  \n",
    "\n",
    "\n",
    "default_value_movie_bias = [(i, avg_movie_biases) for i in range(1, N_movies+1)] \n",
    "default_value_movie_bias_rdd = sc.parallelize(default_value_movie_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_biases_final = (movie_biases\n",
    "                      .rightOuterJoin(default_value_movie_bias_rdd)\n",
    "                      .map(lambda row: (row[0], row[1][0] if row[1][0]!=None else row[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_biases_final.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the score predictions for the pairs (movieId, userId) of the validation_set from the formula mu + movie bias + user bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set = (validation_set\n",
    "                  .map(lambda row: (row[\"userId\"], row[\"movieId\"]))\n",
    "                  .join(user_biases_final) \n",
    "                  .map(lambda row: (row[1][0], (row[0], row[1][1]))) # rdd of tuples (movieId, (userId, user_bias))\n",
    "                  .join(movie_biases_final) \n",
    "                  .map(lambda row: (row[0], row[1][0][0], row[1][0][1], row[1][1])) # rdd of tuples (movieId, userId, user_bias, movie_bias)\n",
    "                  .map(lambda row: ((row[0], row[1]), mu+row[2]+row[3])) # rdd of tuples ((movieId, userId), predictive rating)\n",
    "                 )\n",
    "                  \n",
    "                  \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3.8 : Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we calculate the error on the validation_set, \n",
    "# using the prediction_set calculated previously and the formula in the statement.\n",
    "\n",
    "\n",
    "def error(rdd):\n",
    "    rdd_and_prediction = rdd.map(lambda row: ((row[\"movieId\"], row[\"userId\"]), row[\"rating\"])).join(prediction_set)\n",
    "    compute_error = (rdd_and_prediction\n",
    "                     .map(lambda row: (row[0][1], (1, (row[1][0]-row[1][1])**2)))\n",
    "                     .reduceByKey(lambda row1, row2: (row1[0]+row2[0], row1[1]+row2[1]))\n",
    "                     .map(lambda row: (1, sqrt(row[1][1]/row[1][0])))\n",
    "                     .reduce(lambda row1, row2: (row1[0]+row2[0], row1[1]+row2[1])))\n",
    "    \n",
    "    return compute_error[1]/compute_error[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = error(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
